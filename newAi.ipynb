{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = np.random.default_rng()\n",
    "class Layer:\n",
    "    def __init__(self,inputAmnt,outputAmnt):\n",
    "        def sharmaAct(input):\n",
    "            x = input\n",
    "            mask1 = x < -0.5\n",
    "            mask2 = (-0.5 <= x) & (x <= 0.5)\n",
    "            result = np.zeros_like(x)  # Initialize an array of zeros\n",
    "            result[mask1] = (x[mask1] + 1.5) / np.sqrt(np.e)\n",
    "            result[mask2] = np.exp(x[mask2])\n",
    "            result[~(mask1 | mask2)] = x[~(mask1 | mask2)] + 1\n",
    "            return result\n",
    "        def sharmaActPrime(input):\n",
    "            x = input\n",
    "            mask1 = x < -0.5\n",
    "            mask2 = (-0.5 <= x) & (x <= 0.5)\n",
    "            result = np.zeros_like(x)  # Initialize an array of zeros\n",
    "            result[mask1] =  1 / np.sqrt(np.e) \n",
    "            result[mask2] = np.exp(x[mask2])\n",
    "            result[~(mask1 | mask2)] = 1 \n",
    "            return result\n",
    "        self.f = sharmaAct\n",
    "        self.fPrime = sharmaActPrime\n",
    "        self.w = np.random.uniform(low=-1,high=1,size=(outputAmnt,inputAmnt))\n",
    "        self.b = np.random.uniform(low=-1,high=1,size=(outputAmnt,1))\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        self.linearCombo = 0\n",
    "        self.input = 0\n",
    "    def compute(self,input):\n",
    "        self.input = input\n",
    "        self.linearCombo = self.w @ input + self.b\n",
    "        return self.f(self.linearCombo)\n",
    "    def compPrime(self):\n",
    "        return self.fPrime(self.linearCombo)\n",
    "class inputLayer:\n",
    "    def __init__(self,activation):\n",
    "        def relu(input):\n",
    "            return np.maximum(input,0)\n",
    "        def reluPrime(input):\n",
    "            return np.greater(input, 0).astype(np.float32)\n",
    "        def sigmoid(input):\n",
    "            return 1/(1+np.exp(-input))\n",
    "        def sigmoidPrime(input):\n",
    "            return -np.exp(-input)/(1+np.exp(input))\n",
    "        def tanhPrime(input):\n",
    "            np.power(1/(np.cosh(input)),2)\n",
    "        def sharmaAct(input):\n",
    "            x = input\n",
    "            mask1 = x < -0.5\n",
    "            mask2 = (-0.5 <= x) & (x <= 0.5)\n",
    "\n",
    "            result = np.zeros_like(x)  # Initialize an array of zeros\n",
    "            result[mask1] = (x[mask1] + 1.5) / np.sqrt(np.e)\n",
    "            result[mask2] = np.exp(x[mask2])\n",
    "            result[~(mask1 | mask2)] = x[~(mask1 | mask2)] + 1\n",
    "            return result\n",
    "        def sharmaActPrime(input):\n",
    "            x = input\n",
    "            mask1 = x < -0.5\n",
    "            mask2 = (-0.5 <= x) & (x <= 0.5)\n",
    "            \n",
    "            result = np.zeros_like(x)  # Initialize an array of zeros\n",
    "            result[mask1] =  1 / np.sqrt(np.e) \n",
    "            result[mask2] = np.exp(x[mask2])\n",
    "            result[~(mask1 | mask2)] = 1 \n",
    "            return result\n",
    "        val = {\"relu\":[relu,reluPrime],\"sigmoid\":[sigmoid,sigmoidPrime],\"tanh\":[np.tanh,tanhPrime],'sharma':[sharmaAct,sharmaActPrime]}\n",
    "        self.f = val[activation][0]\n",
    "\n",
    "\n",
    "class ANN:\n",
    "    def __init__(self,input,*layers):\n",
    "        self.layers = list(layers)\n",
    "        self.input = input\n",
    "        self.internalAlpha = 1\n",
    "    def predict(self,input: np.array):\n",
    "        out = self.input.f(input)\n",
    "        for k in self.layers:\n",
    "            out = k.compute(out)\n",
    "        return out/np.sum(np.abs(out))\n",
    "    def gradientDescent(self,inputs,outputs,alpha):\n",
    "        for (input,output) in zip(inputs,outputs):\n",
    "            vectorPart = 2*(self.predict(input)-output) * self.layers[-1].compPrime()\n",
    "            self.layers.reverse()\n",
    "            matThing = np.identity(n=self.layers[0].w.shape[0])\n",
    "            prevW = None\n",
    "            for l in self.layers:\n",
    "                if prevW is not None:\n",
    "                    matThing = matThing @ (prevW * np.transpose(l.compPrime()))\n",
    "                value = np.transpose(matThing) @ vectorPart\n",
    "                change_b = (l.b - alpha/len(inputs) * value)\n",
    "                change_w = (l.w - alpha/len(inputs) * (value*np.tile(np.transpose(l.input),(l.w.shape[0],1))))\n",
    "                if(np.all(np.isfinite(change_b)) and np.all(np.isfinite(change_w))):\n",
    "                    l.b = change_b\n",
    "                    l.w = change_w\n",
    "                else:\n",
    "                    print(\"is overflowing ruh roh\")\n",
    "                prevW = l.w  \n",
    "            self.layers.reverse()\n",
    "    def alternateDescent(self,inputs,outputs,alpha):\n",
    "        for (input,output) in zip(inputs,outputs):\n",
    "            vectorPart = 2*(self.predict(input)-output) * self.layers[-1].compPrime()\n",
    "            self.layers.reverse()\n",
    "            matThing = np.identity(n=self.layers[0].w.shape[0])\n",
    "            prevW = None\n",
    "            for l in self.layers:\n",
    "                if prevW is not None:\n",
    "                    matThing = matThing @ (prevW * np.transpose(l.compPrime()))\n",
    "                value = np.transpose(matThing) @ vectorPart\n",
    "                l.b_grad += alpha * value\n",
    "                l.w_grad += value * np.tile(np.transpose(l.input),(l.w.shape[0],1))*alpha\n",
    "                prevW = l.w    \n",
    "            self.layers.reverse()\n",
    "        for l in self.layers:\n",
    "            l.w -= l.w_grad/len(inputs)\n",
    "            l.b -= l.b_grad/len(inputs)\n",
    "            l.w_grad -= l.w_grad\n",
    "            l.b_grad -= l.b_grad\n",
    "    \n",
    "    \n",
    "        \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(inputLayer('sharma'),Layer(784,15),Layer(15,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], -1,1) / 255  # input for training\n",
    "x_test = x_test.reshape(x_test.shape[0], -1,1) / 255  # input for testing\n",
    "y_train = y_train.reshape(60_000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(input):\n",
    "    z = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "    z[0][input] = 1\n",
    "    return np.transpose(z)\n",
    "output_test = np.apply_along_axis(encode,1,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmas(x):\n",
    "    return 1/(1+max(x,0)/10)\n",
    "for i in range(1):\n",
    "    model.gradientDescent(x_train,output_test,sigmas(i)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5968833333333333\n"
     ]
    }
   ],
   "source": [
    "#0.67935 (its been stable at about 67.5 percennt) 0.5951666666666666\n",
    "def percentAcc(model):\n",
    "    global x_train,y_train\n",
    "    error = 0\n",
    "    for (input,output) in zip(x_train,y_train):\n",
    "        if(np.argmax(model.predict(input)) != output):\n",
    "            error += 1\n",
    "    return error\n",
    "print(1-percentAcc(model)/60_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3044.7478651381316\n"
     ]
    }
   ],
   "source": [
    "def mse(model): #(3044.33517272429)\n",
    "    global x_train,output_test\n",
    "    error = 0\n",
    "    for (input,output) in zip(x_train,output_test):\n",
    "        diff = model.predict(input)-output\n",
    "        error += np.dot(diff[0],diff[0])\n",
    "    return error\n",
    "print(mse(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
